{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the saved models and tokenizers\n",
    "distil_gpt2_model = AutoModelForCausalLM.from_pretrained(\"./trained_modelsF/distil_gpt2\").to(device)\n",
    "distil_gpt2_tokenizer = AutoTokenizer.from_pretrained(\"./trained_modelsF/distil_gpt2_tokenizer\")\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"./trained_modelsF/gpt2\").to(device)\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"./trained_modelsF/gpt2_tokenizer\")\n",
    "gpt_neo_model = AutoModelForCausalLM.from_pretrained(\"./trained_modelsF/gpt_neo\").to(device)\n",
    "gpt_neo_tokenizer = AutoTokenizer.from_pretrained(\"./trained_modelsF/gpt_neo_tokenizer\")\n",
    "\n",
    "# Define a function to generate text using a given model and tokenizer\n",
    "def generate_text(model, tokenizer, max_length=50):\n",
    "    inputs = torch.tensor([[tokenizer.bos_token_id]], device=device)\n",
    "    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, do_sample=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate 500 unique texts for each model\n",
    "num_texts = 500\n",
    "models = [\n",
    "    (distil_gpt2_model, distil_gpt2_tokenizer),\n",
    "    (gpt2_model, gpt2_tokenizer),\n",
    "    (gpt_neo_model, gpt_neo_tokenizer),\n",
    "]\n",
    "\n",
    "generated_texts = []\n",
    "for model, tokenizer in models:\n",
    "    texts = []\n",
    "    for _ in range(num_texts):\n",
    "        text = generate_text(model, tokenizer, max_length=50)\n",
    "        texts.append(text)\n",
    "    generated_texts.append(texts)\n",
    "\n",
    "# Choose the best text outputs using majority voting\n",
    "averaged_outputs = []\n",
    "for i in range(num_texts):\n",
    "    averaged_output = np.mean([tokenizer.encode(texts[i], return_tensors='pt') for _, tokenizer in models], axis=0)\n",
    "    averaged_output = averaged_output.astype(int)  # Convert float to integer\n",
    "    text = distil_gpt2_tokenizer.decode(averaged_output[0], skip_special_tokens=True)\n",
    "    averaged_outputs.append(text)\n",
    "\n",
    "# Save the chosen text outputs to a CSV file\n",
    "csv_file = \"Final-major-voting.csv\"\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Text\"]) \n",
    "    for text in averaged_outputs:\n",
    "        writer.writerow([text])\n",
    "\n",
    "print(f\"{num_texts} unique texts have been generated using the ensemble model and saved to '{csv_file}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
